[
  {
    "chunk_index": 1,
    "title": "Data Preprocessing: An Overview and Data Quality",
    "text": "Chapter 3: Data Preprocessing\n1.\nData Preprocessing:An Overview\na.\nData Quality\nb.\nMajor Tasks in Data Preprocessing\n2.\nData Cleaning\n3.\nData Integration\n4.\nData Reduction\n5.\nData Transformation and Data Discretization\n6.\nSummary\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n2\n\nData Quality: Why Preprocess the Data?\nMeasures for data quality:A multidimensional view\nAccuracy: correct or wrong, accurate or not\nCompleteness: not recorded, unavailable, …\nConsistency: some modified but some not, dangling, …\nTimeliness: timely update?\nBelievability: how trustable the data are correct?\nInterpretability: how easily the data can be understood?\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n3\n\nMajor Tasks in Data Preprocessing\nData cleaning\nFill in missing values, smooth noisy data, identify or remove outliers, and\nresolve inconsistencies\nData integration\nIntegration of multiple databases, data cubes, or files\nData reduction\nDimensionality reduction\nNumerosity reduction\nData compression\nData transformation and data discretization\nNormalization\nConcept hierarchy generation\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n4"
  },
  {
    "chunk_index": 2,
    "title": "Data Cleaning: Concepts and Handling Missing Data",
    "text": "2. Data Cleaning\nData in the Real World Is Dirty: Lots of potentially incorrect data, e.g.,\ninstrument faulty, human or computer error, transmission error\nincomplete: lacking attribute values, lacking certain attributes of interest,\nor containing only aggregate data\ne.g., Occupation=“ ” (missing data)\nnoisy: containing noise, errors, or outliers\ne.g., Salary=“−10” (an error)\ninconsistent: containing discrepancies in codes or names, e.g.,\nAge=“42”, Birthday=“03/07/2010”\nWas rating “1, 2, 3”, now rating “A, B, C”\ndiscrepancy between duplicate records\nIntentional (e.g., disguised missing data)\nJan. 1 as everyone’s birthday?\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n5\n\nIncomplete (Missing) Data\nData is not always available\nE.g., many tuples have no recorded value for several\nattributes, such as customer income in sales data\nMissing data may be due to\nequipment malfunction\ninconsistent with other recorded data and thus deleted\ndata not entered due to misunderstanding\ncertain data may not be considered important at the time of\nentry\nnot register history or changes of the data\nMissing data may need to be inferred\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n6\n\nHow to Handle Missing Data?\nIgnore the tuple: usually done when class label is missing (when\ndoing classification)—not effective when the % of missing\nvalues per attribute varies considerably\nFill in the missing value manually: tedious + infeasible?\nFill in it automatically with\na global constant : e.g.,“unknown”, a new class?!\nthe attribute mean\nthe attribute mean for all samples belonging to the same\nclass: smarter\nthe most probable value: inference-based such as Bayesian\nformula or decision tree\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n7"
  },
  {
    "chunk_index": 3,
    "title": "Handling Noisy Data and Data Cleaning Process",
    "text": "8\nNoisy Data\nNoise: random error or variance in a measured variable\nIncorrect attribute values may be due to\nfaulty data collection instruments\ndata entry problems\ndata transmission problems\ntechnology limitation\ninconsistency in naming convention\nOther data problems which require data cleaning\nduplicate records\nincomplete data\ninconsistent data\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n8\n\nHow to Handle Noisy Data?\nBinning\nfirst sort data and partition into (equal-frequency) bins\nthen one can smooth by bin means, smooth by bin median,\nsmooth by bin boundaries, etc.\nRegression\nsmooth by fitting the data into regression functions\nClustering\ndetect and remove outliers\nCombined computer and human inspection\ndetect suspicious values and check by human (e.g., deal with\npossible outliers)\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n9\n\nData Cleaning as a Process\nData discrepancy detection\nUse metadata (e.g., domain, range, dependency, distribution)\nCheck field overloading\nCheck uniqueness rule, consecutive rule and null rule\nUse commercial tools\nData scrubbing: use simple domain knowledge (e.g., postal code, spell-\ncheck) to detect errors and make corrections\nData auditing: by analyzing data to discover rules and relationship to\ndetect violators (e.g., correlation and clustering to find outliers)\nData migration and integration\nData migration tools: allow transformations to be specified\nETL (Extraction/Transformation/Loading) tools: allow users to specify\ntransformations through a graphical user interface\nIntegration of the two processes\nIterative and interactive (e.g., Potter’sWheels)\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n10"
  },
  {
    "chunk_index": 4,
    "title": "Data Integration and Correlation Analysis",
    "text": "3. Data Integration\nData integration:\nCombines data from multiple sources into a coherent store\nSchema integration: e.g.,A.cust-id B.cust-#\nIntegrate metadata from different sources\nEntity identification problem:\nIdentify real world entities from multiple data sources, e.g., Bill Clinton = William Clinton\nDetecting and resolving data value conflicts\nFor the same real world entity, attribute values from different sources are\ndifferent\nPossible reasons: different representations, different scales, e.g., metric vs.\nBritish units\n11\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n11\n\nHandling Redundancy in Data Integration\nRedundant data occur often when integration of multiple\ndatabases\nObject identification: The same attribute or object may have\ndifferent names in different databases\nDerivable data: One attribute may be a “derived” attribute in\nanother table, e.g., annual revenue\nRedundant attributes may be able to be detected by correlation\nanalysis and covariance analysis\nCareful integration of the data from multiple sources may help\nreduce/avoid redundancies and inconsistencies and improve\nmining speed and quality\n12\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n12\n\nCorrelation Analysis (Nominal Data)\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n\n\n\nExpected\nExpected\nObserved\n2\n2\n)\n(\n\nΧ2 (chi-square) test\nThe larger the Χ2 value, the more likely the variables are \nrelated\nThe cells that contribute the most to the Χ2 value are those \nwhose actual count is very different from the expected count\nCorrelation does not imply causality\n# of hospitals and # of car-theft in a city are correlated\nBoth are causally linked to the third variable: population\n13"
  },
  {
    "chunk_index": 5,
    "title": "Correlation and Covariance Analysis",
    "text": "Chi-Square Calculation: An Example\n93\n.\n507\n840\n)\n840\n1000\n(\n360\n)\n360\n200\n(\n210\n)\n210\n50\n(\n90\n)\n90\n250\n(\n2\n2\n2\n2\n2\n\n\n\n\n\n\n\n\n\n\nΧ2 (chi-square) calculation (numbers in parenthesis are expected \ncounts calculated based on the data distribution in the two \ncategories)\nIt shows that like_science_fiction and play_chess are correlated \nin the group\nPlay chess\nNot play chess\nSum (row)\nLike science fiction\n250(90)\n200(360)\n450\nNot like science fiction\n50(210)\n1000(840)\n1050\nSum(col.)\n300\n1200\n1500\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n14\n\nCorrelation Analysis (Numeric Data)\nB\nA\nn\ni\ni\ni\nB\nA\nn\ni\ni\ni\nB\nA\nn\nB\nA\nn\nb\na\nn\nB\nb\nA\na\nr\n\n\n\n\n)1\n(\n)\n(\n)1\n(\n)\n)(\n(\n1\n1\n,\n\n\n\n\n\n\n\n\n\n\n\nCorrelation coefficient (also called Pearson’s product moment \ncoefficient)\nwhere n is the number of tuples,       and      are the respective means of A \nand B, σA and σB are the respective standard deviation of A and B, and Σ(aibi) \nis the sum of the AB cross-product.\nIf rA,B > 0, A and B are positively correlated (A’s values increase \nas B’s).  The higher, the stronger correlation.\nrA,B = 0: independent;  rAB < 0: negatively correlated\nA\nB\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n15\n\nVisually Evaluating Correlation\nScatter plots \nshowing the \nsimilarity from \n–1 to 1.\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n16"
  },
  {
    "chunk_index": 6,
    "title": "Correlation, Covariance, and Examples",
    "text": "Correlation (viewed as linear relationship)\nCorrelation measures the linear relationship between\nobjects\nTo compute correlation, we standardize data objects, A\nand B, and then take their dot product\n)\n(\n/\n))\n(\n(\n'\nA\nstd\nA\nmean\na\na\nk\nk\n\n\n)\n(\n/\n))\n(\n(\n'\nB\nstd\nB\nmean\nb\nb\nk\nk\n\n\n'\n'\n)\n,\n(\nB\nA\nB\nA\nn\ncorrelatio\n\n\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n17\n\nCovariance (Numeric Data)\nCovariance is similar to correlation\nCorrelation coefficient:\nwhere n is the number of tuples,\nand\nare the respective mean or expected\nvalues of A and B, σA and σB are the respective standard deviation of A and B.\nPositive covariance: If CovA,B > 0, then A and B both tend to be larger than their\nexpected values.\nNegative covariance: If CovA,B < 0 then if A is larger than its expected value, B is likely\nto be smaller than its expected value.\nIndependence: CovA,B = 0 but the converse is not true:\n\nSome pairs of random variables may have a covariance of 0 but are not independent. Only under\nsome additional assumptions (e.g., the data follow multivariate normal distributions) does a\ncovariance of 0 imply independence\nA\nB\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n18\n\nCo-Variance: An Example\nIt can be simplified in computation as\nSuppose two stocks A and B have the following values in one week: (2, 5), (3,\n8), (5, 10), (4, 11), (6, 14).\nQuestion:\nIf the stocks are affected by the same industry trends, will their\nprices rise or fall together?\nE(A) = (2 + 3 + 5 + 4 + 6)/ 5 = 20/5 = 4\nE(B) = (5 + 8 + 10 + 11 + 14) /5 = 48/5 = 9.6\nCov(A,B) = (2×5+3×8+5×10+4×11+6×14)/5 −4 × 9.6 = 4\nThus,A and B rise together since Cov(A, B) > 0.\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n19"
  },
  {
    "chunk_index": 7,
    "title": "Data Reduction Strategies and Dimensionality Reduction",
    "text": "4. Data Reduction Strategies\nData reduction: Obtain a reduced representation of the data set that is\nmuch smaller in volume but yet produces the same (or almost the same)\nanalytical results\nWhy data reduction? — A database/data warehouse may store terabytes of\ndata.\nComplex data analysis may take a very long time to run on the\ncomplete data set.\nData reduction strategies\nDimensionality reduction, e.g., remove unimportant attributes\nWavelet transforms\nPrincipal Components Analysis (PCA)\nFeature subset selection, feature creation\nNumerosity reduction (some simply call it: Data Reduction)\nRegression and Log-Linear Models\nHistograms, clustering, sampling\nData cube aggregation\nData compression\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n20\n\nData Reduction 1: Dimensionality \nReduction\nCurse of dimensionality\n\nWhen dimensionality increases, data becomes increasingly sparse\n\nDensity and distance between points, which is critical to clustering, outlier\nanalysis, becomes less meaningful\n\nThe possible combinations of subspaces will grow exponentially\nDimensionality reduction\n\nAvoid the curse of dimensionality\n\nHelp eliminate irrelevant features and reduce noise\n\nReduce time and space required in data mining\n\nAllow easier visualization\nDimensionality reduction techniques\n\nWavelet transforms\n\nPrincipal Component Analysis\n\nSupervised and nonlinear techniques (e.g., feature selection)\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n21"
  },
  {
    "chunk_index": 8,
    "title": "Attribute Subset Selection and Numerosity Reduction",
    "text": "Attribute Subset Selection\nAnother way to reduce dimensionality of data\nRedundant attributes\nDuplicate much or all of the information contained in one\nor more other attributes\nE.g., purchase price of a product and the amount of sales tax\npaid\nIrrelevant attributes\nContain no information that is useful for the data mining\ntask at hand\nE.g., students' ID is often irrelevant to the task of predicting\nstudents' GPA\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n22\n\nData Reduction 2: Numerosity Reduction\nReduce data volume by choosing alternative, smaller forms of\ndata representation\nParametric methods (e.g., regression)\nAssume\nthe\ndata\nfits\nsome\nmodel,\nestimate\nmodel\nparameters, store only the parameters, and discard the data\n(except possible outliers)\nEx.: Log-linear models—obtain value at a point in m-D space\nas the product on appropriate marginal subspaces\nNon-parametric methods\nDo not assume models\nMajor families: histograms, clustering, sampling, …\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n23"
  },
  {
    "chunk_index": 9,
    "title": "Parametric Data Reduction, Regression, and Clustering",
    "text": "Parametric Data Reduction: Regression and \nLog-Linear Models\nLinear regression\nData modeled to fit a straight line\nOften uses the least-square method to fit the line\nMultiple regression\nAllows a response variable Y to be modeled as a linear\nfunction of multidimensional feature vector\nLog-linear model\nApproximates\ndiscrete\nmultidimensional\nprobability\ndistributions\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n24\n\nRegression Analysis\nRegression analysis: A collective name for\ntechniques for the modeling and analysis of\nnumerical data consisting of values of a\ndependent variable (also called response\nvariable or measurement) and of one or\nmore\nindependent\nvariables\n(aka.\nexplanatory variables or predictors)\nThe parameters are estimated so as to give\na \"best fit\" of the data\nMost commonly the best fit is evaluated by\nusing the least squares method, but other\ncriteria have also been used\nUsed for prediction (including \nforecasting of time-series data), \ninference, hypothesis testing, and \nmodeling of causal relationships\ny\nx\ny = x + 1\nX1\nY1\nY1’\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n25\n\nClustering\nPartition data set into clusters based on similarity, and store\ncluster representation (e.g., centroid and diameter) only\nCan be very effective if data is clustered but not if data is\n“smeared”\nCan have hierarchical clustering and be stored in multi-\ndimensional index tree structures\nThere are many choices of clustering definitions and clustering\nalgorithms\nCluster analysis will be studied in depth in Chapter 10\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n26"
  },
  {
    "chunk_index": 10,
    "title": "Sampling, Data Cube Aggregation, and Data Compression",
    "text": "Sampling\nSampling: obtaining a small sample s to represent the whole\ndata set N\nAllow a mining algorithm to run in complexity that is\npotentially sub-linear to the size of the data\nKey principle: Choose a representative subset of the data\nSimple random sampling may have very poor performance in\nthe presence of skew\nDevelop adaptive sampling methods, e.g., stratified sampling:\nNote: Sampling may not reduce database I/Os (page at a time)\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n27\n\nData Cube Aggregation\nThe lowest level of a data cube (base cuboid)\nThe aggregated data for an individual entity of interest\nE.g., a customer in a phone calling data warehouse\nMultiple levels of aggregation in data cubes\nFurther reduce the size of data to deal with\nReference appropriate levels\nUse the smallest representation which is enough to solve\nthe task\nQueries regarding aggregated information should be answered\nusing data cube, when possible\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n28\n\nData Reduction 3: Data Compression\nString compression\nThere are extensive theories and well-tuned algorithms\nTypically lossless, but only limited manipulation is possible\nwithout expansion\nAudio/video compression\nTypically lossy compression, with progressive refinement\nSometimes small fragments of signal can be reconstructed\nwithout reconstructing the whole\nTime sequence is not audio\nTypically short and vary slowly with time\nDimensionality\nand\nnumerosity\nreduction\nmay\nalso\nbe\nconsidered as forms of data compression\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n29"
  },
  {
    "chunk_index": 11,
    "title": "Data Compression and Data Transformation",
    "text": "Data Compression\nOriginal Data\nCompressed \nData\nlossless\nOriginal Data\nApproximated \nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n30\n\n5. Data Transformation\nA function that maps the entire set of values of a given attribute to a new set\nof replacement values s.t. each old value can be identified with one of the new\nvalues\nMethods\nSmoothing: Remove noise from data\nAttribute/feature construction\nNew attributes constructed from the given ones\nAggregation: Summarization, data cube construction\nNormalization: Scaled to fall within a smaller, specified range\nmin-max normalization\nz-score normalization\nnormalization by decimal scaling\nDiscretization: Concept hierarchy climbing\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n31"
  },
  {
    "chunk_index": 12,
    "title": "Normalization and Discretization",
    "text": "Normalization\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n32\n716\n.0\n0\n)\n0\n0.1(\n000\n,\n12\n000\n,\n98\n000\n,\n12\n600\n,\n73\n\n\n\n\n\nMin-max normalization: to [new_minA, new_maxA]\nEx.  Let income range $12,000 to $98,000 normalized to [0.0, 1.0].  Then \n$73,000 is mapped to  \nZ-score normalization (μ: mean, σ: standard deviation):\nEx. Let μ = 54,000, σ = 16,000.  Then\nNormalization by decimal scaling\n225\n.1\n000\n,\n16\n000\n,\n54\n600\n,\n73\n\n\nA\nA\nA\nA\nA\nA\nmin\nnew\nmin\nnew\nmax\nnew\nmin\nmax\nmin\nv\nv\n_\n)\n_\n_\n(\n'\n\n\n\n\n\nA\nA\nv\nv\n\n\n\n\n'\nj\nv\nv\n10\n'\nWhere j is the smallest integer such that Max(|ν’|) < 1\n\nDiscretization\nThree types of attributes\nNominal—values from an unordered set, e.g., color, profession\nOrdinal—values from an ordered set, e.g., military or academic rank\nNumeric—real numbers, e.g., integer or real numbers\nDiscretization: Divide the range of a continuous attribute into intervals\nInterval labels can then be used to replace actual data values\nReduce data size by discretization\nSupervised vs. unsupervised\nSplit (top-down) vs. merge (bottom-up)\nDiscretization can be performed recursively on an attribute\nPrepare for further analysis, e.g., classification\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n33"
  },
  {
    "chunk_index": 13,
    "title": "Data Discretization Methods and Summary",
    "text": "34\nData Discretization Methods\nTypical methods:All the methods can be applied recursively\nBinning\nTop-down split, unsupervised\nHistogram analysis\nTop-down split, unsupervised\nClustering analysis (unsupervised, top-down split or bottom-\nup merge)\nDecision-tree analysis (supervised, top-down split)\nCorrelation (e.g., 2) analysis (unsupervised, bottom-up\nmerge)\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n34\n\n6. Summary\nData quality: accuracy, completeness, consistency, timeliness, believability,\ninterpretability\nData cleaning: e.g. missing/noisy values, outliers\nData integration from multiple sources:\nEntity identification problem\nRemove redundancies\nDetect inconsistencies\nData reduction\nDimensionality reduction\nNumerosity reduction\nData compression\nData transformation and data discretization\nNormalization\nConcept hierarchy generation\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n38"
  },
  {
    "chunk_index": 14,
    "title": "References",
    "text": "References\n\nD. P. Ballou and G. K. Tayi. Enhancing data quality in data warehouse environments. Comm. of\nACM, 42:73-78, 1999\n\nA. Bruce, D. Donoho, and H.-Y. Gao. Wavelet analysis. IEEE Spectrum, Oct 1996\n\nT. Dasu and T. Johnson. Exploratory Data Mining and Data Cleaning. John Wiley, 2003\n\nJ. Devore and R. Peck. Statistics: The Exploration and Analysis of Data. Duxbury Press, 1997.\n\nH. Galhardas, D. Florescu, D. Shasha, E. Simon, and C.-A. Saita. Declarative data cleaning:\nLanguage, model, and algorithms. VLDB'01\n\nM. Hua and J. Pei. Cleaning disguised missing data: A heuristic approach. KDD'07\n\nH. V. Jagadish, et al., Special Issue on Data Reduction Techniques.\nBulletin of the Technical\nCommittee on Data Engineering, 20(4), Dec. 1997\n\nH. Liu and H. Motoda (eds.). Feature Extraction, Construction, and Selection: A Data Mining\nPerspective. Kluwer Academic, 1998\n\nJ. E. Olson. Data Quality: The Accuracy Dimension. Morgan Kaufmann, 2003\n\nD. Pyle. Data Preparation for Data Mining. Morgan Kaufmann, 1999\n\nV. Raman and J. Hellerstein. Potters Wheel: An Interactive Framework for Data Cleaning and\nTransformation, VLDB’2001\n\nT. Redman. Data Quality: The Field Guide. Digital Press (Elsevier), 2001\n\nR. Wang, V. Storey, and C. Firth. A framework for analysis of data quality research. IEEE Trans.\nKnowledge and Data Engineering, 7:623-640, 1995\nCh3. Data Preprocessing\nDr. Kadan ALJOUMAA\n39"
  }
]